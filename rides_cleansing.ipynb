{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c5c4bd",
   "metadata": {},
   "source": [
    "# Sript: Rides cleansing and rides & weather merging\n",
    "## File: rides_cleansing.ipynb\n",
    "\n",
    "The script reads the files with rides data and stores them in a dictionary of data frames. After that the data frames are cleaned by keeping only the important columns (Date, Time, and Pick up address) and renaming them to maintain consistancy.\n",
    "\n",
    "Then, the dictionary of data frames is collapsed into one data frame and grouped by date, obtaining the count of rides per day.\n",
    "\n",
    "Information of the rides was obtained from the following files:\n",
    "- other-American_B01362.csv\n",
    "- other-Carmel_B00256.csv\n",
    "- other-Diplo_B01196.csv\n",
    "- other-Federal_02216.csv\n",
    "- other-Firstclass_B01536.csv\n",
    "- other-Highclass_B01717.csv\n",
    "- other-Prestige_B01338.csv\n",
    "- other-Skyline_B00111.csv\n",
    "- uber-raw-data-jul14.csv\n",
    "- uber-raw-data-aug14.csv\n",
    "- uber-raw-data-sep14.csv\n",
    "\n",
    "Finally, the rides data frame (merged_df) and the weather data frame (weather_data) is merged into one data frame by date (weather_rides).\n",
    "\n",
    "As a bonus, the final data frame was grouped again by weekday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c970de",
   "metadata": {},
   "source": [
    "### Defining name of the files and aliases for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to read\n",
    "files_name = [\n",
    "    \"other-American_B01362\",\n",
    "    \"other-Carmel_B00256\",\n",
    "    \"other-Diplo_B01196\",\n",
    "    \"other-Federal_02216\",\n",
    "    \"other-Firstclass_B01536\",\n",
    "    \"other-Highclass_B01717\",\n",
    "    \"other-Prestige_B01338\",\n",
    "    \"other-Skyline_B00111\"    \n",
    "]\n",
    "\n",
    "# Shortcut name to refer to each file\n",
    "key_name = [\n",
    "    \"American\",\n",
    "    \"Carmel\",\n",
    "    \"Diplo\",\n",
    "    \"Federal\",\n",
    "    \"Firstclass\",\n",
    "    \"Highclass\",\n",
    "    \"Prestige\",\n",
    "    \"Skyline\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e644a7f",
   "metadata": {},
   "source": [
    "### Storing files into a dictionary of data frames (one key for each file)\n",
    "Not uber files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The 9 files will be saved in 1 dictionary (uber_dict). The key for each file will be the shortcut name saved in\n",
    "#    in the list 'key_name'. Example: To display the data frame of the file \"other-Firstclass_B01536.csv\" type:\n",
    "#    uber_dict[\"Firstclass\"]\n",
    "\n",
    "uber_dict = {}\n",
    "\n",
    "for index, file in enumerate(files_name):\n",
    "\n",
    "    file_path = f\"Uber_Rides_Rawdata/Uber/{file}.csv\"\n",
    "\n",
    "# I found that the files have different encoder so I had to use the try-except functions.\n",
    "    try:\n",
    "        uber_dict[key_name[index]] = pd.read_csv(file_path, encoding = \"utf-8\")\n",
    "        print(f\"GOOD | FILE: {file} | ENCODING: utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            uber_dict[key_name[index]] = pd.read_csv(file_path, encoding = \"cp1252\")\n",
    "            print(f\"GOOD | FILE: {file} | ENCODING: cp1252\")\n",
    "        except:\n",
    "            print(\"ERROR\")\n",
    "\n",
    "# Uncommnet to get a list of columns before cleaning (for every file) =========\n",
    "# print(\"Columns before cleaning\\n\")\n",
    "# for key in uber_dict:\n",
    "#     for index, column in enumerate(uber_dict[key]):\n",
    "#         print(key, index, column)\n",
    "# =============================================================================\n",
    "\n",
    "# Defining the name of the cleaned columns\n",
    "column_date = \"date\"\n",
    "column_time = \"TIME\"\n",
    "column_pickup = \"PU_ADDRESS\"\n",
    "\n",
    "# Renaming the important columns with the names above.\n",
    "for key in uber_dict:\n",
    "    for index, column in enumerate(uber_dict[key]):\n",
    "        if index == 0:\n",
    "            uber_dict[key] = uber_dict[key].rename(columns = {column: column_date})\n",
    "        elif index == 1:\n",
    "            uber_dict[key] = uber_dict[key].rename(columns = {column: column_time})\n",
    "        elif index == 2:\n",
    "            uber_dict[key] = uber_dict[key].rename(columns = {column: column_pickup})\n",
    "\n",
    "    # Dropping any other column except date, time, and pick up address\n",
    "    uber_dict[key] = uber_dict[key][[column_date, column_time, column_pickup]]\n",
    "    \n",
    "    # Formatting date column to date format\n",
    "    uber_dict[key][column_date] = pd.to_datetime(uber_dict[key][column_date])\n",
    "\n",
    "# Uncommnet to get a list of columns after cleaning (for every file) ==========\n",
    "# print(\"\\n\\nColumns after cleaning\\n\")\n",
    "# for key in uber_dict:\n",
    "#     for index, column in enumerate(uber_dict[key]):\n",
    "#         print(key, index, column)\n",
    "# =============================================================================\n",
    "\n",
    "# Collapsing the dictionary of data frames into one data frame grouped by date (using merge)\n",
    "for index, key in enumerate(uber_dict):\n",
    "    if index == 0:\n",
    "        merged_df = uber_dict[key].groupby(column_date).count().copy().reset_index()\n",
    "    else:\n",
    "        merged_df = merged_df.merge(uber_dict[key].groupby(column_date).count().reset_index(), on = column_date, how = \"outer\", suffixes = (f\"_{key_name[index-1]}\", f\"_{key_name[index]}\"))\n",
    "\n",
    "# print(merged_df)\n",
    "\n",
    "# merged_df = merged_df.set_index(column_date)\n",
    "\n",
    "# Dropping Pick-up column and renaming Time column\n",
    "for column in merged_df:\n",
    "    for value in key_name:\n",
    "        if column == f\"{column_pickup}_{value}\":\n",
    "            merged_df = merged_df.drop(column, axis = 1)\n",
    "        elif column == f\"{column_time}_{value}\":\n",
    "            merged_df = merged_df.rename(columns = {column: f\"count_{value}\"})\n",
    "\n",
    "# Column 'count_Federal' required special attention (NaN values and values in double format)\n",
    "merged_df = merged_df.fillna(0)\n",
    "merged_df[\"count_Federal\"] = merged_df[\"count_Federal\"].astype(int)\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f964d",
   "metadata": {},
   "source": [
    "### Storing files into a dictionary of data frames (one key for each file)\n",
    "Uber files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba021e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with the uber files since they were divided by month\n",
    "\n",
    "# List of files to read\n",
    "uberfiles_name = [\n",
    "    \"uber-raw-data-jul14\",\n",
    "    \"uber-raw-data-aug14\",\n",
    "    \"uber-raw-data-sep14\",    \n",
    "]\n",
    "\n",
    "# Creating a data frame for the first file and appending to the same data frame for the rest of files.\n",
    "for index, file in enumerate(uberfiles_name):\n",
    "    \n",
    "    if index == 0:\n",
    "        file_path = f\"Uber_Rides_Rawdata/Uber/{file}.csv\"\n",
    "        try:\n",
    "            uber_df = pd.read_csv(file_path, encoding = \"utf-8\")\n",
    "            print(f\"GOOD | FILE: {file} | ENCODING: utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                uber_df = pd.read_csv(file_path, encoding = \"cp1252\")\n",
    "                print(f\"GOOD | FILE: {file} | ENCODING: cp1252\")\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "    else:\n",
    "        file_path = f\"Uber_Rides_Rawdata/Uber/{file}.csv\"\n",
    "        try: \n",
    "            uber_df = uber_df.append(pd.read_csv(file_path, encoding = \"utf-8\"))\n",
    "            print(f\"GOOD | FILE: {file} | ENCODING: utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                uber_df = uber_df.append(pd.read_csv(file_path, encoding = \"cp1252\"))\n",
    "                print(f\"GOOD | FILE: {file} | ENCODING: cp1252\")\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "\n",
    "# Renaming date column\n",
    "uber_df = uber_df.rename(columns = {uber_df.columns[0]: column_date})\n",
    "\n",
    "# Line to split date column into date and time columns\n",
    "uber_df[[column_date, column_time]] = uber_df[column_date].str.split(\" \", 1, expand = True)\n",
    "\n",
    "# Formatting date column to date format\n",
    "uber_df[column_date] = pd.to_datetime(uber_df[column_date])\n",
    "\n",
    "# Keeping only the date and time columns\n",
    "uber_df = uber_df[[column_date, column_time]]\n",
    "\n",
    "# Grouping by date and counting into a new data frame\n",
    "count_uber = uber_df.groupby(column_date).count().reset_index()\n",
    "\n",
    "# Renaming the final column name to be consistent with merged_df\n",
    "count_uber = count_uber.rename(columns = {count_uber.columns[1]: \"count_Uber\"})\n",
    "count_uber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ac3f0",
   "metadata": {},
   "source": [
    "### Merging uber data frame (count_uber) and not-uber data frames (merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(count_uber, on = column_date, how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f650ead",
   "metadata": {},
   "source": [
    "### Calculating total of rides (uber and no-uber) per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.set_index(column_date)\n",
    "merged_df[\"Total\"] = merged_df.sum(axis = 1)\n",
    "merged_df = merged_df.reset_index()\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0989d",
   "metadata": {},
   "source": [
    "### Saving final rides data frame (ALL) into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P: Saving rides data (merged_df) to a csv file\n",
    "\n",
    "merged_df.to_csv(\"Data/rides_data.csv\", encoding = \"utf-8\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9d03f",
   "metadata": {},
   "source": [
    "### Merging rides data frame (merged_df) and weather data frame (weather_data).\n",
    "The resulting data frame is called 'weather_rides_df'.\n",
    "\n",
    "An extra column with weekday name is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge of rides and weather data frame\n",
    "weather_rides_df = merged_df.copy()\n",
    "weather_rides_df = weather_rides_df.merge(weather_data, left_on = \"date\", right_on = column_date, how = \"outer\")\n",
    "\n",
    "# Adding the weekday column (number of day)\n",
    "weather_rides_df[\"Weekday\"] = weather_rides_df[column_date].dt.weekday\n",
    "\n",
    "# Changing the weekday column (number of day) to 'name of day' using the map function\n",
    "days = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "days_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weather_rides_df[\"Weekday\"] = weather_rides_df[\"Weekday\"].map(days)\n",
    "# weather_rides_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610142f",
   "metadata": {},
   "source": [
    "### Saving weather & rides data frame into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving merged df (weather and rides) to a csv file\n",
    "\n",
    "weather_rides_df.to_csv(\"Data/weather_rides_df.csv\", encoding = \"utf-8\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c140387",
   "metadata": {},
   "source": [
    "### BONUS: Creating the group by weekday data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aea00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_weekday_sum = weather_rides_df[[\"count_American\", \"count_Carmel\", \"count_Diplo\", \"count_Federal\", \"count_Firstclass\", \"count_Highclass\", \"count_Prestige\", \"count_Skyline\", \"count_Uber\", \"Total\", \"Weekday\"]].groupby(\"Weekday\").sum().reset_index()\n",
    "# print(groupby_weekday_sum)\n",
    "groupby_weekday_average = weather_rides_df[[\"maxtempC\", \"avgtempC\", \"uvIndex\", \"precipMM\", \"cloudcover\", \"humidity\", \"weatherDesc\", \"Weekday\"]].groupby(\"Weekday\").mean().reset_index()\n",
    "# print(groupby_weekday_average)\n",
    "\n",
    "weekday_df = groupby_weekday_sum.copy()\n",
    "weekday_df = weekday_df.merge(groupby_weekday_average, on = \"Weekday\", how = \"outer\", suffixes = (f\"_sum\", f\"_mean\")).set_index(\"Weekday\")\n",
    "weekday_df = weekday_df.reindex(days_list)\n",
    "\n",
    "for index, column in enumerate(weekday_df):\n",
    "#     print(index)\n",
    "    if index <= 9:\n",
    "        if index == 9:\n",
    "            weekday_df = weekday_df.rename(columns = {column: f\"sum_Total\"})\n",
    "        elif index == 8:\n",
    "            weekday_df = weekday_df.rename(columns = {column: f\"sum_Uber\"})\n",
    "        else:\n",
    "            weekday_df = weekday_df.rename(columns = {column: f\"sum_{key_name[index]}\"})\n",
    "    else:\n",
    "        weekday_df = weekday_df.rename(columns = {column: f\"avg_{column}\"})\n",
    "\n",
    "\n",
    "weekday_df.to_csv(\"Data/weekday_df.csv\", encoding = \"utf-8\", index = True, header = True)\n",
    "\n",
    "# print(weekday_df)\n",
    "print(\"\\n\\n SUCCESS!!! Script run until completion.\")\n",
    "print(\"\\n Files 'rides_data.csv', 'weather_rides_df.csv', and 'weekday_df.csv' created and saved in folder 'Data'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bootcamp_env] *",
   "language": "python",
   "name": "conda-env-bootcamp_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
